<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><style data-href="/xi-editor-gatsby/1.d39ed622b78079570519.css">/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}main{display:block}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}button,input{font-family:inherit;font-size:100%;line-height:1.15;margin:0;overflow:visible}button{text-transform:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}legend{color:inherit;display:table;max-width:100%;white-space:normal}[type=checkbox],[type=radio],legend{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}[hidden]{display:none}html{box-sizing:border-box;font-family:sans-serif}*,:after,:before{box-sizing:inherit}blockquote,h1,h2,h3,h4,p,pre{margin:0}button{background:transparent;padding:0}button:focus{outline:1px dotted;outline:5px auto -webkit-focus-ring-color}ul{margin:0}*,:after,:before{border:0 solid #dae1e7}img{border-style:solid;max-width:100%;height:auto}input::-webkit-input-placeholder{color:inherit;opacity:.5}input::-ms-input-placeholder{color:inherit;opacity:.5}input::placeholder{color:inherit;opacity:.5}[role=button],button{cursor:pointer}table{border-collapse:collapse}.container{width:100%}@media (min-width:576px){.container{max-width:576px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:992px){.container{max-width:992px}}@media (min-width:1200px){.container{max-width:1200px}}.list-reset{list-style:none;padding:0}.bg-black{background-color:#22292f}.bg-white{background-color:#fff}.bg-green-lighter{background-color:#d9f1e3}.bg-blue-darker{background-color:#20303a}.bg-blue{background-color:#3490dc}.hover\:bg-blue-dark:hover{background-color:#2779bd}.border-green{border-color:#24d06b}.border-green-lighter{border-color:#d9f1e3}.border-green-lightest{border-color:#3efc9c}.focus\:border-grey-light:focus{border-color:#dae1e7}.rounded{border-radius:.25rem}.rounded-lg{border-radius:1.125rem}.rounded-full{border-radius:9999px}.rounded-t-lg{border-top-left-radius:1.125rem;border-top-right-radius:1.125rem}.border{border-width:1px}.border-t-2{border-top-width:2px}.border-t-4{border-top-width:4px}.border-l-8{border-left-width:8px}.block{display:block}.inline-block{display:inline-block}.table{display:table}.hidden{display:none}.flex{display:flex}.inline-flex{display:inline-flex}.flex-col{flex-direction:column}.flex-col-reverse{flex-direction:column-reverse}.items-center{align-items:center}.items-stretch{align-items:stretch}.justify-end{justify-content:flex-end}.justify-center{justify-content:center}.justify-between{justify-content:space-between}.content-end{align-content:flex-end}.flex-1{flex:1 1 0%}.flex-no-shrink{flex-shrink:0}.font-sans{font-family:system-ui,BlinkMacSystemFont,-apple-system,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Fira Sans,Droid Sans,Helvetica Neue,sans-serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-4{height:1rem}.h-6{height:1.5rem}.h-16{height:4rem}.h-32{height:8rem}.h-48{height:12rem}.h-full{height:100%}.leading-tight{line-height:1.25}.mx-4{margin-left:1rem;margin-right:1rem}.mx-5{margin-left:1.25rem;margin-right:1.25rem}.my-8{margin-top:2rem;margin-bottom:2rem}.mx-auto{margin-left:auto;margin-right:auto}.ml-0{margin-left:0}.mb-1{margin-bottom:.25rem}.mt-2{margin-top:.5rem}.ml-2{margin-left:.5rem}.mb-3{margin-bottom:.75rem}.mt-4{margin-top:1rem}.mr-4{margin-right:1rem}.mb-4{margin-bottom:1rem}.ml-4{margin-left:1rem}.mt-5{margin-top:1.25rem}.mb-5{margin-bottom:1.25rem}.mt-6{margin-top:1.5rem}.mt-8{margin-top:2rem}.mb-8{margin-bottom:2rem}.ml-8{margin-left:2rem}.mt-16{margin-top:4rem}.mb-24{margin-bottom:6rem}.mr-32{margin-right:8rem}.mb-32{margin-bottom:8rem}.ml-32{margin-left:8rem}.mb-auto{margin-bottom:auto}.max-h-full{max-height:100%}.max-w-xs{max-width:20rem}.max-w-md{max-width:40rem}.max-w-lg{max-width:50rem}.min-h-full{min-height:100%}.-mb-1{margin-bottom:-.25rem}.-ml-8{margin-left:-2rem}.-mt-16{margin-top:-4rem}.-mt-32{margin-top:-8rem}.-mr-32{margin-right:-8rem}.overflow-hidden{overflow:hidden}.overflow-x-auto{overflow-x:auto}.overflow-y-hidden{overflow-y:hidden}.p-1{padding:.25rem}.p-2{padding:.5rem}.p-4{padding:1rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.px-2{padding-left:.5rem;padding-right:.5rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.py-6{padding-top:1.5rem;padding-bottom:1.5rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.py-8{padding-top:2rem;padding-bottom:2rem}.px-8{padding-left:2rem;padding-right:2rem}.pl-4{padding-left:1rem}.pt-6{padding-top:1.5rem}.pl-8{padding-left:2rem}.pr-10{padding-right:2.5rem}.pb-12{padding-bottom:3rem}.pb-32{padding-bottom:8rem}.absolute{position:absolute}.relative{position:relative}.pin{top:0;bottom:0;left:0}.pin,.pin-r{right:0}.pin-b{bottom:0}.pin-l{left:0}.shadow-md{box-shadow:0 4px 8px 0 rgba(0,0,0,.12),0 2px 4px 0 rgba(0,0,0,.08)}.fill-current{fill:currentColor}.stroke-current{stroke:currentColor}.text-grey-darkest{color:#3d4852}.text-grey-dark{color:#8795a1}.text-white{color:#fff}.text-green{color:#24d06b}.text-green-lighter{color:#d9f1e3}.text-blue-darker{color:#20303a}.hover\:text-green:hover{color:#24d06b}.text-xs{font-size:.75rem}.text-sm{font-size:.875rem}.text-md{font-size:.9375rem}.text-base{font-size:1rem}.text-lg{font-size:1.125rem}.text-2xl{font-size:1.5rem}.text-3xl{font-size:1.875rem}.text-4xl{font-size:2.25rem}.text-5xl{font-size:3rem}.uppercase{text-transform:uppercase}.no-underline{text-decoration:none}.tracking-wide{letter-spacing:.05em}.w-4{width:1rem}.w-6{width:1.5rem}.w-24{width:6rem}.w-48{width:12rem}.w-64{width:16rem}.w-auto{width:auto}.w-full{width:100%}.z-0{z-index:0}.z-10{z-index:10}#___gatsby,#___gatsby>div,body,html{width:100%;height:100%}@media (min-width:768px){.md\:mx-0{margin-left:0;margin-right:0}.md\:mt-0{margin-top:0}.md\:mr-0{margin-right:0}.md\:ml-0{margin-left:0}.md\:mt-8{margin-top:2rem}.md\:mt-16{margin-top:4rem}.md\:-ml-16{margin-left:-4rem}.md\:px-0{padding-left:0;padding-right:0}.md\:px-10{padding-left:2.5rem;padding-right:2.5rem}.md\:text-5xl{font-size:3rem}}@media (min-width:992px){.lg\:block{display:block}.lg\:flex{display:flex}.lg\:flex-row{flex-direction:row}.lg\:flex-row-reverse{flex-direction:row-reverse}.lg\:ml-0{margin-left:0}.lg\:ml-8{margin-left:2rem}.lg\:-mr-32{margin-right:-8rem}.lg\:-ml-32{margin-left:-8rem}.lg\:pl-23{padding-left:5.625rem}.lg\:w-3\/4{width:75%}}@media (min-width:1200px){.xl\:w-4\/5{width:80%}}</style><meta name="generator" content="Gatsby 2.0.73"/><link rel="manifest" href="/xi-editor-gatsby/manifest.webmanifest"/><title data-react-helmet="true">Rope science, part 11 - practical syntax highlighting | Xi</title><link data-react-helmet="true" rel="icon" type="image/png" sizes="32x32" href="/xi-editor-gatsby/logos/favicon-32x32.png"/><link data-react-helmet="true" rel="icon" type="image/png" sizes="16x16" href="/xi-editor-gatsby/logos/favicon-16x16.png"/><link data-react-helmet="true" rel="mask-icon" href="/xi-editor-gatsby/logos/safari-pinned-tab.svg" color="#24d06b"/><link data-react-helmet="true" rel="shortcut icon" href="favicon.ico"/><link data-react-helmet="true" rel="apple-touch-icon" href="/xi-editor-gatsby/logos/apple-touch-icon.png"/><meta data-react-helmet="true" name="msapplication-TileColor"/><meta data-react-helmet="true" name="msapplication-config" content="browserconfig.xml"/><meta data-react-helmet="true" name="description" content="23 Apr 2017 In this post, we present an incremental algorithm for syntax
highlighting. It has very good performance, measured primarily by
latency but also memory usage and power consumption. It does not
require a large amount of code, but the analysis is subtle and
sophisticated. Your favorite codeâ€¦"/><meta data-react-helmet="true" name="image" content="https://elod10.github.io/xi-editor-gatsby/logos/logo.png"/><meta data-react-helmet="true" property="al:ios:url" content="nflx://www.netflix.com/?locale=fr-FR"/><meta data-react-helmet="true" property="al:android:app_name" content="Xi"/><meta data-react-helmet="true" property="al:android:url" content="https://elod10.github.io"/><meta data-react-helmet="true" name="apple-mobile-web-app-capable" content="yes"/><meta data-react-helmet="true" property="og:title" content="Xi"/><meta data-react-helmet="true" property="og:description" content="Xi-Editor website"/><meta data-react-helmet="true" property="og:image" content="https://elod10.github.io/xi-editor-gatsby/logos/logo.png"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" name="twitter:title" content="Xi"/><meta data-react-helmet="true" name="twitter:description" content="Xi-Editor website"/><meta data-react-helmet="true" name="twitter:image" content="https://elod10.github.io/xi-editor-gatsby/logos/logo.png"/><link rel="sitemap" type="application/xml" href="/xi-editor-gatsby/sitemap.xml"/><link as="script" rel="preload" href="/xi-editor-gatsby/1-6e57b76f82a78bcb777f.js"/><link as="script" rel="preload" href="/xi-editor-gatsby/component---cache-gatsby-mdx-mdx-wrappers-dir-6-a-6-d-7612520-aff-73-cb-56-c-0-b-39-c-2-af-0-be-scope-hash-3010-b-3-badc-54-a-9-dfa-4-a-4-c-80-e-419-a-41-b-2-js-96b730ebcfd9805849e8.js"/><link as="script" rel="preload" href="/xi-editor-gatsby/app-46f25a4e0bb303c5d58f.js"/><link as="script" rel="preload" href="/xi-editor-gatsby/0-3058d9943b60ad518acf.js"/><link as="script" rel="preload" href="/xi-editor-gatsby/webpack-runtime-8f000ca1e32cbe792686.js"/><link as="fetch" rel="preload" href="/xi-editor-gatsby/static/d/595/path---documentation-rope-science-11-7-da-fa7-Ra6FiZ0ltEOy3MlDlvJWDEMYc9w.json" crossOrigin="use-credentials"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" role="group"><div class="h-full w-full flex flex-col items-stretch font-sans"><header class="flex flex-col flex-no-shrink border-t-2 border-green h-16 mb-32"><div class="bg-white"><div class="container mx-auto"><div class="flex justify-between items-center py-4"><div class="flex items-center"><a class="text-white no-underline flex items-center ml-2 md:ml-0" href="/xi-editor-gatsby/"><p class="font-bold text-4xl text-blue-darker">Xi</p></a></div><div class="flex justify-between overflow-x-auto overflow-y-hidden mx-4 md:mx-0"><a class="flex items-center no-underline text-base uppercase text-blue-darker mx-5 font-semibold witespace-no-wrap" href="/xi-editor-gatsby/">home</a><a class="flex items-center no-underline text-base uppercase text-blue-darker mx-5 font-semibold witespace-no-wrap" href="/xi-editor-gatsby/documentation/frontend-notes/">docs</a><a class="flex items-center no-underline text-base uppercase text-blue-darker mx-5 font-semibold witespace-no-wrap" href="/xi-editor-gatsby/gsoc/gsoc/">gsoc</a><a class="flex items-center no-underline text-base uppercase text-blue-darker mx-5 font-semibold witespace-no-wrap" href="/xi-editor-gatsby/contribute/">contribute</a><a class="flex items-center no-underline text-base uppercase text-blue-darker mx-5 font-semibold witespace-no-wrap" href="/xi-editor-gatsby/building-docs/">buildind docs</a><a class="flex items-center no-underline text-base uppercase text-blue-darker mx-5 font-semibold witespace-no-wrap" href="/xi-editor-gatsby/blog">blog</a></div></div></div></div></header><main role="main" class="flex-1 -mt-32"><div class="flex"><div class="w-64"><nav class="pt-6 max-w-xs w-auto pb-32"><ul class="list-reset flex flex-col flex-1"><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/frontend-notes/">Notes on writing front-ends</a></li><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/frontend-protocol/">The Frontend Protocol</a></li><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/plugin/">Plugin architecture</a></li><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/config/">Working with the config system</a></li><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/crdt/">CRDT - An approach to async plugins and undo</a></li><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/crdt-details/">CRDT - The Xi Text Engine</a></li><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/fuchsia-ledger-crdts/">CRDT - Using the Ledger for CRDTs</a></li><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/rope-science-00/">Rope science - Introduction</a></li><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/rope-science-01/">Rope science, part 1 - MapReduce for text</a></li><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/rope-science-02/">Rope science, part 2 - metrics</a></li><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/rope-science-03/">Rope science, part 3 - Grapheme cluster boundaries</a></li><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/rope-science-04/">Rope science, part 4 - parenthesis matching</a></li><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/rope-science-05/">Rope science, part 5 - incremental word wrapping</a></li><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/rope-science-06/">Rope science, part 6 - parallel and asynchronous word wrapping</a></li><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/rope-science-08/">Rope science, part 8 - CRDTs for concurrent editing</a></li><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/rope-science-09/">Rope science, part 9 - CRDT Approach to Async Plugins and Undo</a></li><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/rope-science-10/">Rope science, part 10 - designing for a conflict-free world</a></li><li class="leading-tight max-x-xs"><a aria-current="page" class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md font-semibold border-l-8 border-green-lightest" href="/xi-editor-gatsby/documentation/rope-science-11/">Rope science, part 11 - practical syntax highlighting</a></li><li class="leading-tight max-x-xs"><a class="block text-md px-6 py-2 text-blue-darker no-underline hover:text-green rounded-md" href="/xi-editor-gatsby/documentation/rope-science-12/">Rope science, part 12 - minimal invalidation</a></li></ul></nav></div><div class="max-w-lg px-2 md:px-10 py-8 mb-24 content mx-auto lg:ml-8 overflow-hidden"><div class="lg:w-3/4 xl:w-4/5"><h1 class="ml-4 lg:ml-0 text-blue-darker mt-8 mb-4">Rope science, part 11 - practical syntax highlighting</h1><div><div><p><em>23 Apr 2017</em></p><p>In this post, we present an incremental algorithm for syntax
highlighting. It has very good performance, measured primarily by
latency but also memory usage and power consumption. It does not
require a large amount of code, but the analysis is subtle and
sophisticated. Your favorite code editor would almost certainly
benefit from adopting it.</p><p>Pedagogically, this post also gives a case study in systematically
transforming a simple functional program into an incremental
algorithm, meaning an algorithm that takes a delta on input and
produces a delta on output, so that applying that delta gives the same
result as running the entire function from scratch, beginning to end.
Such algorithms are the backbone of xi editor, the basis of
near-instant response even for very large files.</p><h2 id="The-syntax-highlighting-function"><a href="#The-syntax-highlighting-function" aria-hidden="true" class="anchor"></a>The syntax highlighting function</h2><p>Most syntax highlighting schemes (including the TextMate/Sublime/Atom
format) follow this function signature for the basic syntax
highlighting operation (code is in pseudo-rust):</p><pre><code>fn syntax (previous_state, line) -&gt; (next_state, spans);
</code></pre><p>Typically this &quot;state&quot; is a stack of finite states, i.e. this is a
<a href="https://en.wikipedia.org/wiki/Pushdown_automaton" target="_blank" rel="nofollow noopener noreferrer">pushdown
automaton</a>. Such
automata can express a large family of grammars. In fact, the
incredibly general class of <a href="https://en.wikipedia.org/wiki/LR_parser" target="_blank" rel="nofollow noopener noreferrer">LR
parsers</a> could be accomodated
by adding one additional token of lookahead in addition to the line,
a fairly straightforward extension to this algorithm.</p><p>I won&#x27;t go into more detail about the syntax function itself; within
this framework, the algorithms described in this post are entirely
generic.</p><h2 id="A-batch-algorithm"><a href="#A-batch-algorithm" aria-hidden="true" class="anchor"></a>A batch algorithm</h2><p>The simplest algorithm to apply syntax highlighting to a file is to
run the function on each line from beginning to end:</p><pre><code>let mut state = State::initial();
for line in input_file.lines() {
    let (new_state, spans) = syntax(state, line);
    output.render(line, spans);
    state = new_state;
}
</code></pre><p>This algorithm has some appealing properties. In addition to being
quite simple, it also has minimal memory requirements: one line of
text, plus whatever state is required by the syntax function. It&#x27;s
useful for highlighting a file on initial load, and also for
applications such as statically generating documentation files.</p><p>For this post, it&#x27;s also something of a correctness spec; all the
fancy stuff we do has to give the same answer in the end.</p><h2 id="Random-access-caching"><a href="#Random-access-caching" aria-hidden="true" class="anchor"></a>Random access; caching</h2><p>Let&#x27;s say we&#x27;re not processing the file in batch mode, but will be
displaying it in a window with ability to scroll to a random point,
and want to be able to compute the highlighting on the fly. In
particular, let&#x27;s say we don&#x27;t want to store all the spans for the
whole file. Even in a compact representation, such spans are
comparable to the size of the input text, potentially much more.</p><p>We can write the following functional program:</p><pre><code>fn get_state(file, line_number) -&gt; state {
    file.iter_lines(0, line_number).fold(
        State::initial(),
        |state, line| syntax(state, line).state
    )
}

fn get_spans(file, line_number) -&gt; spans {
    let state = get_state(file, line_number);
    syntax(state, file.get_line(line_number)).spans
}
</code></pre><p>This will work very well for lines near the beginning of the file,
but has a serious performance problem; it is O(n) to retrieve one
line&#x27;s worth of spans, so O(n^2) to process the file.</p><p>Fortunately, <a href="https://en.wikipedia.org/wiki/Memoization" target="_blank" rel="nofollow noopener noreferrer">memoization</a>,
a traditional technique for optimizing functional programs, can come
to the rescue. Storing the intermediate results of <code>get_state</code> reduces
the runtime back to O(n). We also see the algorithm start to become
incremental, in that it&#x27;s possible to render the first screen of the
file quickly, without having to process the whole thing.</p><p>However, these benefits come at a cost, namely the memory required
to store the intermediate results. In this case, we only need store
the state per line (which, in a compact representation, need only be
one machine word), so it might be acceptable. But to handle extremely
large files, we might want to do better.</p><p>One good compromise would be to use a <em>cache</em> with only partial
coverage of the <code>get_state</code> function; when the cache overflows, we
evict some entry in the cache to make room. Then, to compute
<code>get_state</code> for an arbitrary line, we find closest previous cache
entry, and run the fold forward from there.</p><p>This cache is a classic speed/space tradeoff. The amount of time to
compute a query is essentially proportional to the <em>gap length</em>
between one entry and the next. For random access patterns, it follows
that the optimal pattern would be evenly spaced entries. Then the
time required for a query is O(n/m), where m is the cache size.</p><p>Tuning such a cache, in particular choosing a cache replacement
strategy, is tricky. We&#x27;ll defer discussion of that for later.</p><h2 id="Handling-mutation"><a href="#Handling-mutation" aria-hidden="true" class="anchor"></a>Handling mutation</h2><p>Of course, we <em>really</em> want to be able to do interactive syntax
highlighting on a file being edited. Fortunately, the above cache can
be extended to handle this use case as well.</p><p>As the file is mutated, existing cache entries might become <em>invalid.</em>
We define a cache entry (line<em>number, state) as being <!-- -->_<!-- -->valid</em> if that
state is actually equal to computing <code>get_state(line_number)</code> from
scratch. Editing a line need not only change the spans for that line;
it might cause state changes that ripple down from there. A classic
example would be inserting <code>/*</code> to open a comment; then the entire
rest of the file would be rendered as a comment. So, unlike a typical
cache, changing one line might invalidate an arbitrary fraction of the
cache contents.</p><p>We augment the cache with a <em>frontier,</em> a set of cache entries. All
operations maintain the following invariant:</p><p><em>If a cache entry is valid and it is not in the frontier, then the
next entry in the cache is also valid.</em></p><p>From this invariant immediately follows a number of useful properties.
All lines up to the first element of the frontier are valid. Thus, if
the frontier is empty, the entire cache is valid.</p><p>This invariant is carefully designed so that it can be easily restored
after an editing operation, specifically that all operations take
minimal time (I <em>think</em> it&#x27;s O(1) amortized, but establishing that
would take careful analysis).</p><p>Specifically, after changing the contents of a single line, it
suffices to add the closest previous cache entry to the frontier.
Other editing operations are similarly easy; to replace an arbitrary
region of text, also delete cache entries for which the starts of the
lines are in strictly in the interior of the region. For inserts and
deletes, the line numbers after the edit will also need to be fixed
up.</p><p>Of course, it&#x27;s not enough to properly invalidate the cache, it&#x27;s also
important to make progress towards re-validating it. Here is the
algorithm to do one granule of work:</p><ul><li>Take the first element of the frontier. It refers to a cache entry:
<code>(line_number, state)</code>.</li><li>Evaluate <code>syntax(state, file.get_line(line_number))</code>, resulting in a
new_state.</li><li>If <code>line_number + 1</code> does not have an entry in the cache, or if it
does and the entry&#x27;s state != new_state, then insert
<code>(line_number + 1, new_state)</code> into the cache, and move this element
of the frontier to that entry.</li><li>Otherwise, just delete this element from the frontier.</li></ul><p>The only other subtle operation is deleting an entry from the cache
(especially evictions). If that entry is in the frontier, then the
element of the frontier must be moved to the previous entry.</p><h2 id="On-the-representation-of-the-frontier"><a href="#On-the-representation-of-the-frontier" aria-hidden="true" class="anchor"></a>On the representation of the frontier</h2><p>It&#x27;s tempting to truncate the frontier, rather than storing it as a
set. In particular, it&#x27;s perfectly correct to just store it as a
reference to the first entry. Then, the operation of adding an element
to the frontier reduces to just taking the minimum.</p><p>However, this temptation should be resisted. Let&#x27;s say the user opens
a comment at the beginning of a large file. The frontier slowly
ripples through the file, recomputing highlighting so that all lines
are in a &quot;commented&quot; state. Then say the user closes the comment when
the frontier is about halfway through the file. This edit will cause
a new frontier to ripple down, restoring the uncommented state. With
the full set representation of the frontier, the old position halfway
through the file will be retained, and when the new frontier reaches
it, states will match, so processing can stop.</p><p>If that old position were not retained, then the frontier would need
to ripple all the way to the end of the file before there would be
confidence the entire cache was valid. So, for a relatively small cost
of maintaining the frontier as a set, we get a pretty nice
optimization, which will improve power consumption and also latency
(the editor can respond more quickly when it has quiesced as opposed
to doing computation in the background).</p><h2 id="Tuning-the-cache"><a href="#Tuning-the-cache" aria-hidden="true" class="anchor"></a>Tuning the cache</h2><p>This is where the rocket science starts. Please check your flight
harnesses.</p><h3 id="Access-patterns"><a href="#Access-patterns" aria-hidden="true" class="anchor"></a>Access patterns</h3><p>Before we can start tuning the cache, we have to characterize the
access patterns. In an interactive editing session, the workload will
consist of a mix of three fundamental patterns: sequential, local, and
random.</p><p>Sequential is familiar from the first algorithm we presented. It&#x27;s an
important case when first loading a file. It will also happen when
edits (such as changing comment balance) cause state changes to ripple
through the file. The cache is basically irrelevant to this access
pattern; the computation has to happen in any case, so the only
purpose of the cache is not to have significant overhead.</p><p>By &quot;local,&quot; we mean edits within a small region of the file, typically
around one screenful. Most such edits <em>won&#x27;t</em> cause extensive state
changes, in fact should result in re-highlighting of just a line or
two. In this access pattern, we want our algorithm to recompute tiny
deltas, so the cache should be <em>dense,</em> meaning that the gap between
the closest previous cache entry and the line being edited be zero or
very small.</p><p>The random access pattern is the most difficult for a cache to deal
with. The best we can possibly do is O(n/m), as above. We expect these
cases to be rare compared with the other two, but it is still
important to have reasonable worst-case behavior.</p><p>Any given editing session will consist of all three of these patterns,
interleaved, in some relative proportions. This is significant for
designing a well-tuned cache, especially because processing some work
from one pattern may leave the cache in poor condition for the next.</p><h3 id="Analyzing-the-cache-performance"><a href="#Analyzing-the-cache-performance" aria-hidden="true" class="anchor"></a>Analyzing the cache performance</h3><p>In most applications, cache performance is characterized almost
entirely by its <em>hit rate,</em> meaning the probability that any given
query will be present in the cache. Most <a href="https://en.wikipedia.org/wiki/Cache_replacement_policies" target="_blank" rel="nofollow noopener noreferrer">cache eviction
policies</a>
are chosen to optimize this quantity.</p><p>However, for this algorithm, the cost of a cache miss is highly
dependent on the <em>gap</em> between entries, and the goal should be to
minimize this gap.</p><p>From this viewpoint, we can see that the LRU (least recently used)
policy, while fine for local access patterns, is absolutely worst case
when mixing sequential with anything else; after sequential procesing,
the cache will consist of a dense block (often at the end of the
file), with a huge gap between the beginning of the file and that
block. As Dan Luu&#x27;s excellent <a href="http://danluu.com/2choices-eviction/" target="_blank" rel="nofollow noopener noreferrer">case
study</a> points out, LRU can also
have this kind of pathological performance in more traditional
applications such as memory hierarchies.</p><p>For the &quot;random&quot; access pattern, the metric we care about is maximum
gap; this establishes a worst case. For LRU, it is O(n), which is
terrible. We want to do better.</p><p>The obvious next eviction policy candidate to consider is randomized.
In traditional cache applications, random eviction fixes the pathology
with perfectly sequential workloads, and performs reasonably well
overall (in Dan&#x27;s analysis, it is better than LRU for some real-world
workloads, worse in others, and in no case has a hit rate more than
about 10% different).</p><p>I tried simulating it [TODO: a more polished version of this document</p><p>would contain lots of beautiful visualizations, plus a cleaned up</p><p>version of the simulation code], and the maximum-gap metric was
horrible, almost as bad as it can get. In scanning the file from
beginning to end, in the final state the entries near the beginning
are decimated; a typical result is that the first entry remaining in
the cache is about halfway through the file.</p><p>For a purely random workload, an ideal replacement policy would be to
choose the entry with the smallest gap between previous and next
entries. A bit of analysis shows that this policy would yield a
maximum gap of 2n/m in the worst case. However, it won&#x27;t perform well
for local access patterns - basically, the state of the cache will
become stuck, as lines most recently added are likely to also have the
smallest gap. Thus, local edits will still have a cost around n/m
lines re-highlighted. It doesn&#x27;t make sense to optimize for the random
case at the expense of the local one.</p><p>Inspired by Dan&#x27;s post, I sought a hybrid. My proposed cache eviction
policy is to probe some small number k of random candidates, and of
those choose the one with the smallest gap as defined above. In my
simulations [TODO: I know, this really needs graphs; what I have now</p><p>is too rough], it performs <em>excellently.</em></p><p>There&#x27;s no obvious best choice of k, it&#x27;s a tradeoff between the
expected mix of local (where smaller is better) and random (where
larger is better). However, there seems to be a magic threshold of 5;
for any smaller value, the maximum gap grows very quickly with the
file size, but for 5 or larger it levels off. In a simulation of an
8k entry cache and a sequential scan through an 8M line file, k=5
yielded a maximum gap of ~9k lines (keep in mind that 2k is the best
possible result here). Beyond that, increasing k doesn&#x27;t have dramatic
consequences, even at k=10 this metric improves only to ~3600, and
that&#x27;s at the expense of degrading local access patterns.</p><p>Obviously it&#x27;s possible to do a more rigorous analysis and more
fine-tuning, but my gut feeling is that this very simple policy will
perform within a small factor of anything more sophisticated; I&#x27;d be
shocked if any policy could improve performance more than a doubling
of the cache size, and with the cache sizes I have in mind, that
should be well affordable. And a larger cache size always has the
advantage that any file with a number of lines that fits entirely
within the cache will have perfect effectiveness.</p><h3 id="Cache-size-and-representation"><a href="#Cache-size-and-representation" aria-hidden="true" class="anchor"></a>Cache size and representation</h3><p>Choosing cache size is always a tradeoff between cache effectiveness
(whether hit rate or maximum-gap) and the cost of the cache itself.
A larger cache should increase effectiveness, but how much?</p><p>This is an empirical question, but we can try to analyze it. Cache
effectiveness is irrelevant for sequential access. For the local case,
it would be reasonable to expect that the &quot;working set&quot; is quite
small, typically on the order of 1000 lines or so.</p><p>And for the random case, the cache only has to perform reasonably
well; we expect these cases to be rare.</p><p>From this, we can guess that the cache doesn&#x27;t have to be very large
to be effective. Thus, a very simple representation is a dense vector
of entries. Some operations (such as deletion and fixup of line
numbers) are O(m) in the size of the cache, but with a very good
constant factor due to the vector representation. So, while it&#x27;s
tempting to use a fancy O(log m) data structure such as a B-tree, this
is probably a case where simpler is better.</p><p>My gut feeling is that a fixed maximum size of 10k entries will yield
near-optimal results in all cases.</p><h3 id="Implementation-state-and-summary"><a href="#Implementation-state-and-summary" aria-hidden="true" class="anchor"></a>Implementation state and summary</h3><p>I haven&#x27;t implemented this yet (beyond the simulations), but really
look forward to it.</p><p>Based on my analysis, this algorithm should provide truly excellent
performance, producing minimal deltas with very modest memory
requirements. I&#x27;m also pleased that the code and data structures are
relatively easy; I have considered <em>much</em> more sophisticated
approaches (including of course my beloved balanced-tree
representation for the cache), which in analysis wouldn&#x27;t perform
nearly as well.</p><p>I think it would be interesting to do a more rigorous analysis. It&#x27;s
possible this technique has already been investigated somewhere, but
I&#x27;m not aware of it; I&#x27;d <em>love</em> to find such a literature.</p><p>Thanks to <a href="https://github.com/cmyr" target="_blank" rel="nofollow noopener noreferrer">Colin Rofls</a> for stimulating
discussions about caching in plugins that inspired many of these
ideas.</p></div></div></div></div></div></main></div></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.page={"componentChunkName":"component---cache-gatsby-mdx-mdx-wrappers-dir-6-a-6-d-7612520-aff-73-cb-56-c-0-b-39-c-2-af-0-be-scope-hash-3010-b-3-badc-54-a-9-dfa-4-a-4-c-80-e-419-a-41-b-2-js","jsonName":"documentation-rope-science-11-7da","path":"/documentation/rope-science-11/"};window.dataPath="595/path---documentation-rope-science-11-7-da-fa7-Ra6FiZ0ltEOy3MlDlvJWDEMYc9w";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"app":["/app-46f25a4e0bb303c5d58f.js"],"component---node-modules-gatsby-plugin-offline-app-shell-js":["/component---node-modules-gatsby-plugin-offline-app-shell-js-5bf3dc2c26ecbd63fbd0.js"],"component---src-templates-blog-list-js":["/component---src-templates-blog-list-js-68e29daae0f560473896.js"],"component---cache-gatsby-mdx-mdx-wrappers-dir-d-95-ab-767-d-696-bb-926-c-5-d-1-d-8-ce-389204-d-scope-hash-3010-b-3-badc-54-a-9-dfa-4-a-4-c-80-e-419-a-41-b-2-js":["/component---cache-gatsby-mdx-mdx-wrappers-dir-d-95-ab-767-d-696-bb-926-c-5-d-1-d-8-ce-389204-d-scope-hash-3010-b-3-badc-54-a-9-dfa-4-a-4-c-80-e-419-a-41-b-2-js-3cce64ab6ad8a00f8053.js"],"component---cache-gatsby-mdx-mdx-wrappers-dir-6-a-6-d-7612520-aff-73-cb-56-c-0-b-39-c-2-af-0-be-scope-hash-3010-b-3-badc-54-a-9-dfa-4-a-4-c-80-e-419-a-41-b-2-js":["/component---cache-gatsby-mdx-mdx-wrappers-dir-6-a-6-d-7612520-aff-73-cb-56-c-0-b-39-c-2-af-0-be-scope-hash-3010-b-3-badc-54-a-9-dfa-4-a-4-c-80-e-419-a-41-b-2-js-96b730ebcfd9805849e8.js"],"component---cache-gatsby-mdx-mdx-wrappers-dir-248-e-51405-d-8-ad-0-e-8-ff-1-a-314-a-5-b-224-cc-0-scope-hash-3010-b-3-badc-54-a-9-dfa-4-a-4-c-80-e-419-a-41-b-2-js":["/component---cache-gatsby-mdx-mdx-wrappers-dir-248-e-51405-d-8-ad-0-e-8-ff-1-a-314-a-5-b-224-cc-0-scope-hash-3010-b-3-badc-54-a-9-dfa-4-a-4-c-80-e-419-a-41-b-2-js-b8d3395483f2c4f148de.js"],"component---src-pages-404-js":["/component---src-pages-404-js-db6cd2ae89009ad1afcd.js"],"component---src-pages-building-docs-js":["/component---src-pages-building-docs-js-d39a25107e95f1a6f7cc.js"],"component---src-pages-contribute-js":["/component---src-pages-contribute-js-c18274c924e984ba4a11.js"],"component---src-pages-index-js":["/component---src-pages-index-js-b6c47ee8616466477674.js"]};/*]]>*/</script><script src="/xi-editor-gatsby/webpack-runtime-8f000ca1e32cbe792686.js" async=""></script><script src="/xi-editor-gatsby/0-3058d9943b60ad518acf.js" async=""></script><script src="/xi-editor-gatsby/app-46f25a4e0bb303c5d58f.js" async=""></script><script src="/xi-editor-gatsby/component---cache-gatsby-mdx-mdx-wrappers-dir-6-a-6-d-7612520-aff-73-cb-56-c-0-b-39-c-2-af-0-be-scope-hash-3010-b-3-badc-54-a-9-dfa-4-a-4-c-80-e-419-a-41-b-2-js-96b730ebcfd9805849e8.js" async=""></script><script src="/xi-editor-gatsby/1-6e57b76f82a78bcb777f.js" async=""></script></body></html>